__file__ =  /home/local/ASUAD/sgopal28/workspace/HASA_PolicyComputation/src/SAMDP_HillClimbing_policy_iteration_runner.py
PARAMETERS
REWARD_NOISE_RANGE =  1.0
NUM_STATES_PER_ROW 3
WEIGHT_FOR_TRUE_POLICY_VALUE 0.0
INCLUDE_TRUE_POLICY_IN_SEARCH False
INCLUDE_EXPECTED_CONFUSED_POLICY_IN_SEARCH True
ENABLE_1HOT_POLICY_REGULARIZATION 0
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
complexity_weight 0
no-op reward(cost) -0.1
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
trial_idx =  0
__file__ =  /home/local/ASUAD/sgopal28/workspace/HASA_PolicyComputation/src/SAMDP_policy_iteration_STANDARD_value_computation.py
Number of full state set iterations = 1
Number of full state set iterations = 2
Number of full state set iterations = 3
Number of full state set iterations = 4
Number of full state set iterations = 5
Number of full state set iterations = 6
policy matrix tensor([[0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]])
Time taken (seconds) =  0.2643394470214844
true_policy_matrix
tensor([[0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]])
[["('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)"]]
===============
weighted value =  tensor(6.4064, dtype=torch.float64)
1-hot policy, true_value =  tensor(60.3807)
1-hot policy, expected_value =  tensor(57.6579, dtype=torch.float64)
TRUE CONFUSION SCORE =  tensor(0.0382, dtype=torch.float64)
Time taken (seconds) =  0.26583242416381836
==========
trial_idx =  1
__file__ =  /home/local/ASUAD/sgopal28/workspace/HASA_PolicyComputation/src/SAMDP_policy_iteration_STANDARD_value_computation.py
Number of full state set iterations = 1
Number of full state set iterations = 2
Number of full state set iterations = 3
Number of full state set iterations = 4
Number of full state set iterations = 5
Number of full state set iterations = 6
policy matrix tensor([[0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.]])
Time taken (seconds) =  0.2669384479522705
true_policy_matrix
tensor([[0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.]])
[["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)"]]
===============
weighted value =  tensor(6.4291, dtype=torch.float64)
1-hot policy, true_value =  tensor(60.5932)
1-hot policy, expected_value =  tensor(57.8616, dtype=torch.float64)
TRUE CONFUSION SCORE =  tensor(0.0382, dtype=torch.float64)
Time taken (seconds) =  0.26840925216674805
==========
trial_idx =  2
__file__ =  /home/local/ASUAD/sgopal28/workspace/HASA_PolicyComputation/src/SAMDP_policy_iteration_STANDARD_value_computation.py
Number of full state set iterations = 1
Number of full state set iterations = 2
Number of full state set iterations = 3
Number of full state set iterations = 4
Number of full state set iterations = 5
Number of full state set iterations = 6
Number of full state set iterations = 7
Number of full state set iterations = 8
policy matrix tensor([[0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]])
Time taken (seconds) =  0.3493647575378418
true_policy_matrix
tensor([[0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]])
[["('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)"]]
===============
weighted value =  tensor(6.4064, dtype=torch.float64)
1-hot policy, true_value =  tensor(60.3807)
1-hot policy, expected_value =  tensor(57.6579, dtype=torch.float64)
TRUE CONFUSION SCORE =  tensor(0.0382, dtype=torch.float64)
Time taken (seconds) =  0.3509092330932617
==========
trial_idx =  3
__file__ =  /home/local/ASUAD/sgopal28/workspace/HASA_PolicyComputation/src/SAMDP_policy_iteration_STANDARD_value_computation.py
Number of full state set iterations = 1
Number of full state set iterations = 2
Number of full state set iterations = 3
Number of full state set iterations = 4
Number of full state set iterations = 5
Number of full state set iterations = 6
policy matrix tensor([[0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]])
Time taken (seconds) =  0.2593574523925781
true_policy_matrix
tensor([[0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]])
[["('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)"]]
===============
weighted value =  tensor(6.4064, dtype=torch.float64)
1-hot policy, true_value =  tensor(60.3807)
1-hot policy, expected_value =  tensor(57.6579, dtype=torch.float64)
TRUE CONFUSION SCORE =  tensor(0.0382, dtype=torch.float64)
Time taken (seconds) =  0.2609269618988037
==========
trial_idx =  4
__file__ =  /home/local/ASUAD/sgopal28/workspace/HASA_PolicyComputation/src/SAMDP_policy_iteration_STANDARD_value_computation.py
Number of full state set iterations = 1
Number of full state set iterations = 2
Number of full state set iterations = 3
Number of full state set iterations = 4
Number of full state set iterations = 5
Number of full state set iterations = 6
Number of full state set iterations = 7
Number of full state set iterations = 8
Number of full state set iterations = 9
policy matrix tensor([[0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.]])
Time taken (seconds) =  0.39931464195251465
true_policy_matrix
tensor([[0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.]])
[["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)"]]
===============
weighted value =  tensor(6.4291, dtype=torch.float64)
1-hot policy, true_value =  tensor(60.5932)
1-hot policy, expected_value =  tensor(57.8616, dtype=torch.float64)
TRUE CONFUSION SCORE =  tensor(0.0382, dtype=torch.float64)
Time taken (seconds) =  0.4008138179779053
==========
trial_idx =  5
__file__ =  /home/local/ASUAD/sgopal28/workspace/HASA_PolicyComputation/src/SAMDP_policy_iteration_STANDARD_value_computation.py
Number of full state set iterations = 1
Number of full state set iterations = 2
Number of full state set iterations = 3
Number of full state set iterations = 4
Number of full state set iterations = 5
Number of full state set iterations = 6
Number of full state set iterations = 7
Number of full state set iterations = 8
Number of full state set iterations = 9
policy matrix tensor([[0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.]])
Time taken (seconds) =  0.3940753936767578
true_policy_matrix
tensor([[0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.]])
[["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)"]]
===============
weighted value =  tensor(6.4291, dtype=torch.float64)
1-hot policy, true_value =  tensor(60.5932)
1-hot policy, expected_value =  tensor(57.8616, dtype=torch.float64)
TRUE CONFUSION SCORE =  tensor(0.0382, dtype=torch.float64)
Time taken (seconds) =  0.395587682723999
==========
trial_idx =  6
__file__ =  /home/local/ASUAD/sgopal28/workspace/HASA_PolicyComputation/src/SAMDP_policy_iteration_STANDARD_value_computation.py
Number of full state set iterations = 1
Number of full state set iterations = 2
Number of full state set iterations = 3
Number of full state set iterations = 4
policy matrix tensor([[0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]])
Time taken (seconds) =  0.17474126815795898
true_policy_matrix
tensor([[0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]])
[["('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)"]]
===============
weighted value =  tensor(6.3997, dtype=torch.float64)
1-hot policy, true_value =  tensor(60.4081)
1-hot policy, expected_value =  tensor(57.5977, dtype=torch.float64)
TRUE CONFUSION SCORE =  tensor(0.0493, dtype=torch.float64)
Time taken (seconds) =  0.17619633674621582
==========
trial_idx =  7
__file__ =  /home/local/ASUAD/sgopal28/workspace/HASA_PolicyComputation/src/SAMDP_policy_iteration_STANDARD_value_computation.py
Number of full state set iterations = 1
Number of full state set iterations = 2
Number of full state set iterations = 3
Number of full state set iterations = 4
Number of full state set iterations = 5
Number of full state set iterations = 6
Number of full state set iterations = 7
policy matrix tensor([[0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.]])
Time taken (seconds) =  0.30729126930236816
true_policy_matrix
tensor([[0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.]])
[["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)"]]
===============
weighted value =  tensor(6.4291, dtype=torch.float64)
1-hot policy, true_value =  tensor(60.5932)
1-hot policy, expected_value =  tensor(57.8616, dtype=torch.float64)
TRUE CONFUSION SCORE =  tensor(0.0382, dtype=torch.float64)
Time taken (seconds) =  0.3087890148162842
==========
trial_idx =  8
__file__ =  /home/local/ASUAD/sgopal28/workspace/HASA_PolicyComputation/src/SAMDP_policy_iteration_STANDARD_value_computation.py
Number of full state set iterations = 1
Number of full state set iterations = 2
Number of full state set iterations = 3
Number of full state set iterations = 4
policy matrix tensor([[0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]])
Time taken (seconds) =  0.18561291694641113
true_policy_matrix
tensor([[0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]])
[["('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)"]]
===============
weighted value =  tensor(6.3997, dtype=torch.float64)
1-hot policy, true_value =  tensor(60.4081)
1-hot policy, expected_value =  tensor(57.5977, dtype=torch.float64)
TRUE CONFUSION SCORE =  tensor(0.0493, dtype=torch.float64)
Time taken (seconds) =  0.18712210655212402
==========
trial_idx =  9
__file__ =  /home/local/ASUAD/sgopal28/workspace/HASA_PolicyComputation/src/SAMDP_policy_iteration_STANDARD_value_computation.py
Number of full state set iterations = 1
Number of full state set iterations = 2
Number of full state set iterations = 3
Number of full state set iterations = 4
Number of full state set iterations = 5
Number of full state set iterations = 6
Number of full state set iterations = 7
Number of full state set iterations = 8
policy matrix tensor([[0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]])
Time taken (seconds) =  0.3680555820465088
true_policy_matrix
tensor([[0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]])
[["('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)"]]
===============
weighted value =  tensor(6.4064, dtype=torch.float64)
1-hot policy, true_value =  tensor(60.3807)
1-hot policy, expected_value =  tensor(57.6579, dtype=torch.float64)
TRUE CONFUSION SCORE =  tensor(0.0382, dtype=torch.float64)
Time taken (seconds) =  0.3694307804107666
==========
trial_idx =  10
__file__ =  /home/local/ASUAD/sgopal28/workspace/HASA_PolicyComputation/src/SAMDP_policy_iteration_STANDARD_value_computation.py
Number of full state set iterations = 1
Number of full state set iterations = 2
Number of full state set iterations = 3
Number of full state set iterations = 4
Number of full state set iterations = 5
Number of full state set iterations = 6
Number of full state set iterations = 7
policy matrix tensor([[0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]])
Time taken (seconds) =  0.3037528991699219
true_policy_matrix
tensor([[0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]])
[["('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)"]]
===============
weighted value =  tensor(6.4064, dtype=torch.float64)
1-hot policy, true_value =  tensor(60.3807)
1-hot policy, expected_value =  tensor(57.6579, dtype=torch.float64)
TRUE CONFUSION SCORE =  tensor(0.0382, dtype=torch.float64)
Time taken (seconds) =  0.3052175045013428
==========
trial_idx =  11
__file__ =  /home/local/ASUAD/sgopal28/workspace/HASA_PolicyComputation/src/SAMDP_policy_iteration_STANDARD_value_computation.py
Number of full state set iterations = 1
Number of full state set iterations = 2
Number of full state set iterations = 3
Number of full state set iterations = 4
Number of full state set iterations = 5
policy matrix tensor([[0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]])
Time taken (seconds) =  0.21573400497436523
true_policy_matrix
tensor([[0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]])
[["('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)"]]
===============
weighted value =  tensor(6.3997, dtype=torch.float64)
1-hot policy, true_value =  tensor(60.4081)
1-hot policy, expected_value =  tensor(57.5977, dtype=torch.float64)
TRUE CONFUSION SCORE =  tensor(0.0493, dtype=torch.float64)
Time taken (seconds) =  0.21726584434509277
==========
trial_idx =  12
__file__ =  /home/local/ASUAD/sgopal28/workspace/HASA_PolicyComputation/src/SAMDP_policy_iteration_STANDARD_value_computation.py
Number of full state set iterations = 1
Number of full state set iterations = 2
Number of full state set iterations = 3
Number of full state set iterations = 4
Number of full state set iterations = 5
Number of full state set iterations = 6
Number of full state set iterations = 7
policy matrix tensor([[0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.]])
Time taken (seconds) =  0.3070998191833496
true_policy_matrix
tensor([[0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.]])
[["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)"]]
===============
weighted value =  tensor(6.4291, dtype=torch.float64)
1-hot policy, true_value =  tensor(60.5932)
1-hot policy, expected_value =  tensor(57.8616, dtype=torch.float64)
TRUE CONFUSION SCORE =  tensor(0.0382, dtype=torch.float64)
Time taken (seconds) =  0.3085765838623047
==========
trial_idx =  13
__file__ =  /home/local/ASUAD/sgopal28/workspace/HASA_PolicyComputation/src/SAMDP_policy_iteration_STANDARD_value_computation.py
Number of full state set iterations = 1
Number of full state set iterations = 2
Number of full state set iterations = 3
Number of full state set iterations = 4
Number of full state set iterations = 5
Number of full state set iterations = 6
Number of full state set iterations = 7
policy matrix tensor([[0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]])
Time taken (seconds) =  0.30576539039611816
true_policy_matrix
tensor([[0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]])
[["('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)"]]
===============
weighted value =  tensor(6.4064, dtype=torch.float64)
1-hot policy, true_value =  tensor(60.3807)
1-hot policy, expected_value =  tensor(57.6579, dtype=torch.float64)
TRUE CONFUSION SCORE =  tensor(0.0382, dtype=torch.float64)
Time taken (seconds) =  0.30727148056030273
==========
trial_idx =  14
__file__ =  /home/local/ASUAD/sgopal28/workspace/HASA_PolicyComputation/src/SAMDP_policy_iteration_STANDARD_value_computation.py
Number of full state set iterations = 1
Number of full state set iterations = 2
Number of full state set iterations = 3
Number of full state set iterations = 4
Number of full state set iterations = 5
policy matrix tensor([[0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]])
Time taken (seconds) =  0.2163527011871338
true_policy_matrix
tensor([[0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]])
[["('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)"]]
===============
weighted value =  tensor(6.4064, dtype=torch.float64)
1-hot policy, true_value =  tensor(60.3807)
1-hot policy, expected_value =  tensor(57.6579, dtype=torch.float64)
TRUE CONFUSION SCORE =  tensor(0.0382, dtype=torch.float64)
Time taken (seconds) =  0.2178030014038086
==========
trial_idx =  15
__file__ =  /home/local/ASUAD/sgopal28/workspace/HASA_PolicyComputation/src/SAMDP_policy_iteration_STANDARD_value_computation.py
Number of full state set iterations = 1
Number of full state set iterations = 2
Number of full state set iterations = 3
Number of full state set iterations = 4
Number of full state set iterations = 5
Number of full state set iterations = 6
Number of full state set iterations = 7
policy matrix tensor([[0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.]])
Time taken (seconds) =  0.3003711700439453
true_policy_matrix
tensor([[0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.]])
[["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)"]]
===============
weighted value =  tensor(6.4291, dtype=torch.float64)
1-hot policy, true_value =  tensor(60.5932)
1-hot policy, expected_value =  tensor(57.8616, dtype=torch.float64)
TRUE CONFUSION SCORE =  tensor(0.0382, dtype=torch.float64)
Time taken (seconds) =  0.30172038078308105
==========
trial_idx =  16
__file__ =  /home/local/ASUAD/sgopal28/workspace/HASA_PolicyComputation/src/SAMDP_policy_iteration_STANDARD_value_computation.py
Number of full state set iterations = 1
Number of full state set iterations = 2
Number of full state set iterations = 3
Number of full state set iterations = 4
Number of full state set iterations = 5
Number of full state set iterations = 6
Number of full state set iterations = 7
Number of full state set iterations = 8
Number of full state set iterations = 9
policy matrix tensor([[0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.]])
Time taken (seconds) =  0.39403319358825684
true_policy_matrix
tensor([[0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.]])
[["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)"]]
===============
weighted value =  tensor(6.4291, dtype=torch.float64)
1-hot policy, true_value =  tensor(60.5932)
1-hot policy, expected_value =  tensor(57.8616, dtype=torch.float64)
TRUE CONFUSION SCORE =  tensor(0.0382, dtype=torch.float64)
Time taken (seconds) =  0.3956167697906494
==========
trial_idx =  17
__file__ =  /home/local/ASUAD/sgopal28/workspace/HASA_PolicyComputation/src/SAMDP_policy_iteration_STANDARD_value_computation.py
Number of full state set iterations = 1
Number of full state set iterations = 2
Number of full state set iterations = 3
Number of full state set iterations = 4
Number of full state set iterations = 5
Number of full state set iterations = 6
Number of full state set iterations = 7
Number of full state set iterations = 8
Number of full state set iterations = 9
Number of full state set iterations = 10
policy matrix tensor([[0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.]])
Time taken (seconds) =  0.44011950492858887
true_policy_matrix
tensor([[0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.]])
[["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)"]]
===============
weighted value =  tensor(6.4291, dtype=torch.float64)
1-hot policy, true_value =  tensor(60.5932)
1-hot policy, expected_value =  tensor(57.8616, dtype=torch.float64)
TRUE CONFUSION SCORE =  tensor(0.0382, dtype=torch.float64)
Time taken (seconds) =  0.44165968894958496
==========
trial_idx =  18
__file__ =  /home/local/ASUAD/sgopal28/workspace/HASA_PolicyComputation/src/SAMDP_policy_iteration_STANDARD_value_computation.py
Number of full state set iterations = 1
Number of full state set iterations = 2
Number of full state set iterations = 3
Number of full state set iterations = 4
Number of full state set iterations = 5
policy matrix tensor([[0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]])
Time taken (seconds) =  0.21695661544799805
true_policy_matrix
tensor([[0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]])
[["('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)"]]
===============
weighted value =  tensor(6.3997, dtype=torch.float64)
1-hot policy, true_value =  tensor(60.4081)
1-hot policy, expected_value =  tensor(57.5977, dtype=torch.float64)
TRUE CONFUSION SCORE =  tensor(0.0493, dtype=torch.float64)
Time taken (seconds) =  0.21844482421875
==========
trial_idx =  19
__file__ =  /home/local/ASUAD/sgopal28/workspace/HASA_PolicyComputation/src/SAMDP_policy_iteration_STANDARD_value_computation.py
Number of full state set iterations = 1
Number of full state set iterations = 2
Number of full state set iterations = 3
Number of full state set iterations = 4
Number of full state set iterations = 5
Number of full state set iterations = 6
Number of full state set iterations = 7
Number of full state set iterations = 8
Number of full state set iterations = 9
policy matrix tensor([[0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.]])
Time taken (seconds) =  0.3874952793121338
true_policy_matrix
tensor([[0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.]])
[["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)"]]
===============
weighted value =  tensor(6.4291, dtype=torch.float64)
1-hot policy, true_value =  tensor(60.5932)
1-hot policy, expected_value =  tensor(57.8616, dtype=torch.float64)
TRUE CONFUSION SCORE =  tensor(0.0382, dtype=torch.float64)
Time taken (seconds) =  0.389019250869751
==========
trial_idx =  20
__file__ =  /home/local/ASUAD/sgopal28/workspace/HASA_PolicyComputation/src/SAMDP_policy_iteration_STANDARD_value_computation.py
Number of full state set iterations = 1
Number of full state set iterations = 2
Number of full state set iterations = 3
Number of full state set iterations = 4
Number of full state set iterations = 5
Number of full state set iterations = 6
Number of full state set iterations = 7
policy matrix tensor([[0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.]])
Time taken (seconds) =  0.33898258209228516
true_policy_matrix
tensor([[0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.]])
[["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)"]]
===============
weighted value =  tensor(6.4291, dtype=torch.float64)
1-hot policy, true_value =  tensor(60.5932)
1-hot policy, expected_value =  tensor(57.8616, dtype=torch.float64)
TRUE CONFUSION SCORE =  tensor(0.0382, dtype=torch.float64)
Time taken (seconds) =  0.3405330181121826
==========
trial_idx =  21
__file__ =  /home/local/ASUAD/sgopal28/workspace/HASA_PolicyComputation/src/SAMDP_policy_iteration_STANDARD_value_computation.py
Number of full state set iterations = 1
Number of full state set iterations = 2
Number of full state set iterations = 3
Number of full state set iterations = 4
Number of full state set iterations = 5
Number of full state set iterations = 6
Number of full state set iterations = 7
Number of full state set iterations = 8
policy matrix tensor([[0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]])
Time taken (seconds) =  0.3650693893432617
true_policy_matrix
tensor([[0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]])
[["('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)"]]
===============
weighted value =  tensor(6.4064, dtype=torch.float64)
1-hot policy, true_value =  tensor(60.3807)
1-hot policy, expected_value =  tensor(57.6579, dtype=torch.float64)
TRUE CONFUSION SCORE =  tensor(0.0382, dtype=torch.float64)
Time taken (seconds) =  0.3665282726287842
==========
trial_idx =  22
__file__ =  /home/local/ASUAD/sgopal28/workspace/HASA_PolicyComputation/src/SAMDP_policy_iteration_STANDARD_value_computation.py
Number of full state set iterations = 1
Number of full state set iterations = 2
Number of full state set iterations = 3
Number of full state set iterations = 4
Number of full state set iterations = 5
policy matrix tensor([[0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.]])
Time taken (seconds) =  0.21468615531921387
true_policy_matrix
tensor([[0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.]])
[["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)"]]
===============
weighted value =  tensor(6.4291, dtype=torch.float64)
1-hot policy, true_value =  tensor(60.5932)
1-hot policy, expected_value =  tensor(57.8616, dtype=torch.float64)
TRUE CONFUSION SCORE =  tensor(0.0382, dtype=torch.float64)
Time taken (seconds) =  0.21616768836975098
==========
trial_idx =  23
__file__ =  /home/local/ASUAD/sgopal28/workspace/HASA_PolicyComputation/src/SAMDP_policy_iteration_STANDARD_value_computation.py
Number of full state set iterations = 1
Number of full state set iterations = 2
Number of full state set iterations = 3
Number of full state set iterations = 4
Number of full state set iterations = 5
Number of full state set iterations = 6
Number of full state set iterations = 7
Number of full state set iterations = 8
Number of full state set iterations = 9
policy matrix tensor([[0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.]])
Time taken (seconds) =  0.39275121688842773
true_policy_matrix
tensor([[0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.]])
[["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)"]]
===============
weighted value =  tensor(6.4291, dtype=torch.float64)
1-hot policy, true_value =  tensor(60.5932)
1-hot policy, expected_value =  tensor(57.8616, dtype=torch.float64)
TRUE CONFUSION SCORE =  tensor(0.0382, dtype=torch.float64)
Time taken (seconds) =  0.3942549228668213
==========
trial_idx =  24
__file__ =  /home/local/ASUAD/sgopal28/workspace/HASA_PolicyComputation/src/SAMDP_policy_iteration_STANDARD_value_computation.py
Number of full state set iterations = 1
Number of full state set iterations = 2
Number of full state set iterations = 3
Number of full state set iterations = 4
Number of full state set iterations = 5
Number of full state set iterations = 6
policy matrix tensor([[0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.]])
Time taken (seconds) =  0.25850868225097656
true_policy_matrix
tensor([[0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.]])
[["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)"]]
===============
weighted value =  tensor(6.4291, dtype=torch.float64)
1-hot policy, true_value =  tensor(60.5932)
1-hot policy, expected_value =  tensor(57.8616, dtype=torch.float64)
TRUE CONFUSION SCORE =  tensor(0.0382, dtype=torch.float64)
Time taken (seconds) =  0.26000523567199707
==========
trial_idx =  25
__file__ =  /home/local/ASUAD/sgopal28/workspace/HASA_PolicyComputation/src/SAMDP_policy_iteration_STANDARD_value_computation.py
Number of full state set iterations = 1
Number of full state set iterations = 2
Number of full state set iterations = 3
Number of full state set iterations = 4
Number of full state set iterations = 5
Number of full state set iterations = 6
Number of full state set iterations = 7
Number of full state set iterations = 8
Number of full state set iterations = 9
Number of full state set iterations = 10
policy matrix tensor([[0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.]])
Time taken (seconds) =  0.4752311706542969
true_policy_matrix
tensor([[0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.]])
[["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)"]]
===============
weighted value =  tensor(6.4291, dtype=torch.float64)
1-hot policy, true_value =  tensor(60.5932)
1-hot policy, expected_value =  tensor(57.8616, dtype=torch.float64)
TRUE CONFUSION SCORE =  tensor(0.0382, dtype=torch.float64)
Time taken (seconds) =  0.4768228530883789
==========
trial_idx =  26
__file__ =  /home/local/ASUAD/sgopal28/workspace/HASA_PolicyComputation/src/SAMDP_policy_iteration_STANDARD_value_computation.py
Number of full state set iterations = 1
Number of full state set iterations = 2
Number of full state set iterations = 3
Number of full state set iterations = 4
Number of full state set iterations = 5
Number of full state set iterations = 6
Number of full state set iterations = 7
policy matrix tensor([[0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]])
Time taken (seconds) =  0.3053607940673828
true_policy_matrix
tensor([[0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]])
[["('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)"]]
===============
weighted value =  tensor(6.4064, dtype=torch.float64)
1-hot policy, true_value =  tensor(60.3807)
1-hot policy, expected_value =  tensor(57.6579, dtype=torch.float64)
TRUE CONFUSION SCORE =  tensor(0.0382, dtype=torch.float64)
Time taken (seconds) =  0.3067967891693115
==========
trial_idx =  27
__file__ =  /home/local/ASUAD/sgopal28/workspace/HASA_PolicyComputation/src/SAMDP_policy_iteration_STANDARD_value_computation.py
Number of full state set iterations = 1
Number of full state set iterations = 2
Number of full state set iterations = 3
Number of full state set iterations = 4
Number of full state set iterations = 5
Number of full state set iterations = 6
Number of full state set iterations = 7
Number of full state set iterations = 8
policy matrix tensor([[0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.]])
Time taken (seconds) =  0.3478240966796875
true_policy_matrix
tensor([[0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.]])
[["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)"]]
===============
weighted value =  tensor(6.4291, dtype=torch.float64)
1-hot policy, true_value =  tensor(60.5932)
1-hot policy, expected_value =  tensor(57.8616, dtype=torch.float64)
TRUE CONFUSION SCORE =  tensor(0.0382, dtype=torch.float64)
Time taken (seconds) =  0.34932589530944824
==========
trial_idx =  28
__file__ =  /home/local/ASUAD/sgopal28/workspace/HASA_PolicyComputation/src/SAMDP_policy_iteration_STANDARD_value_computation.py
Number of full state set iterations = 1
Number of full state set iterations = 2
Number of full state set iterations = 3
Number of full state set iterations = 4
Number of full state set iterations = 5
Number of full state set iterations = 6
Number of full state set iterations = 7
Number of full state set iterations = 8
Number of full state set iterations = 9
policy matrix tensor([[0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.]])
Time taken (seconds) =  0.3921806812286377
true_policy_matrix
tensor([[0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.]])
[["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)"]]
===============
weighted value =  tensor(6.4291, dtype=torch.float64)
1-hot policy, true_value =  tensor(60.5932)
1-hot policy, expected_value =  tensor(57.8616, dtype=torch.float64)
TRUE CONFUSION SCORE =  tensor(0.0382, dtype=torch.float64)
Time taken (seconds) =  0.3936750888824463
==========
trial_idx =  29
__file__ =  /home/local/ASUAD/sgopal28/workspace/HASA_PolicyComputation/src/SAMDP_policy_iteration_STANDARD_value_computation.py
Number of full state set iterations = 1
Number of full state set iterations = 2
Number of full state set iterations = 3
Number of full state set iterations = 4
Number of full state set iterations = 5
Number of full state set iterations = 6
Number of full state set iterations = 7
policy matrix tensor([[0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]])
Time taken (seconds) =  0.3067619800567627
true_policy_matrix
tensor([[0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]])
[["('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)"]]
===============
weighted value =  tensor(6.4064, dtype=torch.float64)
1-hot policy, true_value =  tensor(60.3807)
1-hot policy, expected_value =  tensor(57.6579, dtype=torch.float64)
TRUE CONFUSION SCORE =  tensor(0.0382, dtype=torch.float64)
Time taken (seconds) =  0.30832600593566895
==========
 The best value from hill climbing was =  tensor(57.8616, dtype=torch.float64)
-------------BRANCH AND BOUND ---------------------------------

Using non-default solver options:
 - best_objective: tensor(56.8616, dtype=torch.float64) (default: -inf)

Starting branch & bound solve:
 - dispatcher pid: 3634216 (en4129772l)
 - worker processes: 1
--------------------------------------------------------------------------------------------------------------------------
         Nodes        |                      Objective Bounds                       |              Work              
      Expl    Unexpl  |      Incumbent           Bound    Rel. Gap         Abs. Gap | Time (s)  Nodes/Sec Imbalance   Idle
         0         1  |       56.86156             inf         inf%             inf |      0.0       0.00     0.00%      0
         1         4  |       56.86156        60.62349    6.615938%      3.76192548 |      0.0     286.01     0.00%      0
*      107        18  |       57.76719        58.02225    0.441526%    0.2550574341 |      0.3     310.00     0.00%      0
*      111        14  |       57.86303        57.97295    0.189970%    0.1099222717 |      0.3    1213.72     0.00%      0
       125         0  |       57.86303        57.86303    0.000000%               0 |      0.4     820.56     0.00%      0
--------------------------------------------------------------------------------------------------------------------------

Absolute optimality tolerance met
Optimal solution found!

solver results:
 - solution_status: optimal
 - termination_condition: optimality
 - objective: 57.86303
 - bound: 57.86303
 - absolute_gap: 0
 - relative_gap: 0
 - nodes: 125
 - wall_time: 364.86 ms
 - best_node: Node(objective=57.86303)

Number of Workers:        1
Load Imbalance:       0.00%
 - min: 125 (proc rank=0)
 - max: 125 (proc rank=0)
Average Worker Timing:
 - queue:       2.59% [avg time:  75.5 us, count: 125]
 - load_state:  0.07% [avg time:   2.0 us, count: 125]
 - bound:      93.50% [avg time:   2.7 ms, count: 125]
 - objective:   0.67% [avg time:  51.6 us, count:  47]
 - branch:      0.41% [avg time:  33.0 us, count:  45]
 - other:       2.77% [avg time:  81.0 us, count: 125]

---BNB takes 0.36545300483703613 seconds ---
results from BnB with HASA BnB controller
tensor([[0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.]])
[["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)"]]
Expected Value with HASA BnB Controller = tensor(57.8616, dtype=torch.float64)
time taken for all settings =  9.877500057220459
