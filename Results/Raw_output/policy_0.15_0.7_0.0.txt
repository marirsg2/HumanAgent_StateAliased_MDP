__file__ =  /home/local/ASUAD/sgopal28/workspace/AAAI20222_MDPminimizForHumans/src/SAMDP_HillClimbing_policy_iteration_runner.py
PARAMETERS
REWARD_NOISE_RANGE =  0.0
NUM_STATES_PER_ROW 3
WEIGHT_FOR_TRUE_POLICY_VALUE 0.0
INCLUDE_TRUE_POLICY_IN_SEARCH False
INCLUDE_EXPECTED_CONFUSED_POLICY_IN_SEARCH True
ENABLE_1HOT_POLICY_REGULARIZATION 0
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
complexity_weight 0
no-op reward(cost) -0.1
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
trial_idx =  0
__file__ =  /home/local/ASUAD/sgopal28/workspace/AAAI20222_MDPminimizForHumans/src/SAMDP_policy_iteration_STANDARD_value_computation.py
Number of full state set iterations = 1
Number of full state set iterations = 2
Number of full state set iterations = 3
Number of full state set iterations = 4
Number of full state set iterations = 5
Number of full state set iterations = 6
Number of full state set iterations = 7
policy matrix tensor([[0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]])
Time taken (seconds) =  0.2989792823791504
true_policy_matrix
tensor([[0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]])
[["('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)"]]
===============
weighted value =  tensor(6.4844, dtype=torch.float64)
1-hot policy, true_value =  tensor(61.2179)
1-hot policy, expected_value =  tensor(58.3596, dtype=torch.float64)
TRUE CONFUSION SCORE =  tensor(0.0382, dtype=torch.float64)
Time taken (seconds) =  0.3006153106689453
==========
trial_idx =  1
__file__ =  /home/local/ASUAD/sgopal28/workspace/AAAI20222_MDPminimizForHumans/src/SAMDP_policy_iteration_STANDARD_value_computation.py
Number of full state set iterations = 1
Number of full state set iterations = 2
Number of full state set iterations = 3
Number of full state set iterations = 4
Number of full state set iterations = 5
Number of full state set iterations = 6
Number of full state set iterations = 7
Number of full state set iterations = 8
policy matrix tensor([[0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.]])
Time taken (seconds) =  0.3417999744415283
true_policy_matrix
tensor([[0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.]])
[["('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)"]]
===============
weighted value =  tensor(6.4720, dtype=torch.float64)
1-hot policy, true_value =  tensor(61.2179)
1-hot policy, expected_value =  tensor(58.2479, dtype=torch.float64)
TRUE CONFUSION SCORE =  tensor(0.0493, dtype=torch.float64)
Time taken (seconds) =  0.3431856632232666
==========
trial_idx =  2
__file__ =  /home/local/ASUAD/sgopal28/workspace/AAAI20222_MDPminimizForHumans/src/SAMDP_policy_iteration_STANDARD_value_computation.py
Number of full state set iterations = 1
Number of full state set iterations = 2
Number of full state set iterations = 3
Number of full state set iterations = 4
Number of full state set iterations = 5
Number of full state set iterations = 6
Number of full state set iterations = 7
policy matrix tensor([[0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.]])
Time taken (seconds) =  0.2999098300933838
true_policy_matrix
tensor([[0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.]])
[["('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)"]]
===============
weighted value =  tensor(6.4720, dtype=torch.float64)
1-hot policy, true_value =  tensor(61.2179)
1-hot policy, expected_value =  tensor(58.2479, dtype=torch.float64)
TRUE CONFUSION SCORE =  tensor(0.0493, dtype=torch.float64)
Time taken (seconds) =  0.30130767822265625
==========
trial_idx =  3
__file__ =  /home/local/ASUAD/sgopal28/workspace/AAAI20222_MDPminimizForHumans/src/SAMDP_policy_iteration_STANDARD_value_computation.py
Number of full state set iterations = 1
Number of full state set iterations = 2
Number of full state set iterations = 3
Number of full state set iterations = 4
Number of full state set iterations = 5
Number of full state set iterations = 6
Number of full state set iterations = 7
Number of full state set iterations = 8
policy matrix tensor([[0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]])
Time taken (seconds) =  0.3452425003051758
true_policy_matrix
tensor([[0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]])
[["('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)"]]
===============
weighted value =  tensor(6.4844, dtype=torch.float64)
1-hot policy, true_value =  tensor(61.2179)
1-hot policy, expected_value =  tensor(58.3596, dtype=torch.float64)
TRUE CONFUSION SCORE =  tensor(0.0382, dtype=torch.float64)
Time taken (seconds) =  0.3466305732727051
==========
trial_idx =  4
__file__ =  /home/local/ASUAD/sgopal28/workspace/AAAI20222_MDPminimizForHumans/src/SAMDP_policy_iteration_STANDARD_value_computation.py
Number of full state set iterations = 1
Number of full state set iterations = 2
Number of full state set iterations = 3
Number of full state set iterations = 4
Number of full state set iterations = 5
Number of full state set iterations = 6
policy matrix tensor([[0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.]])
Time taken (seconds) =  0.25408267974853516
true_policy_matrix
tensor([[0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.]])
[["('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)"]]
===============
weighted value =  tensor(6.4720, dtype=torch.float64)
1-hot policy, true_value =  tensor(61.2179)
1-hot policy, expected_value =  tensor(58.2479, dtype=torch.float64)
TRUE CONFUSION SCORE =  tensor(0.0493, dtype=torch.float64)
Time taken (seconds) =  0.2554910182952881
==========
trial_idx =  5
__file__ =  /home/local/ASUAD/sgopal28/workspace/AAAI20222_MDPminimizForHumans/src/SAMDP_policy_iteration_STANDARD_value_computation.py
Number of full state set iterations = 1
Number of full state set iterations = 2
Number of full state set iterations = 3
Number of full state set iterations = 4
Number of full state set iterations = 5
Number of full state set iterations = 6
Number of full state set iterations = 7
policy matrix tensor([[0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.]])
Time taken (seconds) =  0.29625773429870605
true_policy_matrix
tensor([[0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.]])
[["('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)"]]
===============
weighted value =  tensor(6.4720, dtype=torch.float64)
1-hot policy, true_value =  tensor(61.2179)
1-hot policy, expected_value =  tensor(58.2479, dtype=torch.float64)
TRUE CONFUSION SCORE =  tensor(0.0493, dtype=torch.float64)
Time taken (seconds) =  0.29764366149902344
==========
trial_idx =  6
__file__ =  /home/local/ASUAD/sgopal28/workspace/AAAI20222_MDPminimizForHumans/src/SAMDP_policy_iteration_STANDARD_value_computation.py
Number of full state set iterations = 1
Number of full state set iterations = 2
Number of full state set iterations = 3
Number of full state set iterations = 4
Number of full state set iterations = 5
Number of full state set iterations = 6
Number of full state set iterations = 7
Number of full state set iterations = 8
policy matrix tensor([[0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.]])
Time taken (seconds) =  0.33812499046325684
true_policy_matrix
tensor([[0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.]])
[["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)"]]
===============
weighted value =  tensor(6.4844, dtype=torch.float64)
1-hot policy, true_value =  tensor(61.2179)
1-hot policy, expected_value =  tensor(58.3596, dtype=torch.float64)
TRUE CONFUSION SCORE =  tensor(0.0382, dtype=torch.float64)
Time taken (seconds) =  0.339536190032959
==========
trial_idx =  7
__file__ =  /home/local/ASUAD/sgopal28/workspace/AAAI20222_MDPminimizForHumans/src/SAMDP_policy_iteration_STANDARD_value_computation.py
Number of full state set iterations = 1
Number of full state set iterations = 2
Number of full state set iterations = 3
Number of full state set iterations = 4
Number of full state set iterations = 5
Number of full state set iterations = 6
policy matrix tensor([[0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.]])
Time taken (seconds) =  0.25449252128601074
true_policy_matrix
tensor([[0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.]])
[["('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)"]]
===============
weighted value =  tensor(6.4720, dtype=torch.float64)
1-hot policy, true_value =  tensor(61.2179)
1-hot policy, expected_value =  tensor(58.2479, dtype=torch.float64)
TRUE CONFUSION SCORE =  tensor(0.0493, dtype=torch.float64)
Time taken (seconds) =  0.25585198402404785
==========
trial_idx =  8
__file__ =  /home/local/ASUAD/sgopal28/workspace/AAAI20222_MDPminimizForHumans/src/SAMDP_policy_iteration_STANDARD_value_computation.py
Number of full state set iterations = 1
Number of full state set iterations = 2
Number of full state set iterations = 3
Number of full state set iterations = 4
Number of full state set iterations = 5
Number of full state set iterations = 6
Number of full state set iterations = 7
Number of full state set iterations = 8
policy matrix tensor([[0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]])
Time taken (seconds) =  0.3471195697784424
true_policy_matrix
tensor([[0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]])
[["('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)"]]
===============
weighted value =  tensor(6.4720, dtype=torch.float64)
1-hot policy, true_value =  tensor(61.2179)
1-hot policy, expected_value =  tensor(58.2479, dtype=torch.float64)
TRUE CONFUSION SCORE =  tensor(0.0493, dtype=torch.float64)
Time taken (seconds) =  0.3485243320465088
==========
trial_idx =  9
__file__ =  /home/local/ASUAD/sgopal28/workspace/AAAI20222_MDPminimizForHumans/src/SAMDP_policy_iteration_STANDARD_value_computation.py
Number of full state set iterations = 1
Number of full state set iterations = 2
Number of full state set iterations = 3
Number of full state set iterations = 4
Number of full state set iterations = 5
Number of full state set iterations = 6
policy matrix tensor([[0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.]])
Time taken (seconds) =  0.2546689510345459
true_policy_matrix
tensor([[0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.]])
[["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)"]]
===============
weighted value =  tensor(6.4844, dtype=torch.float64)
1-hot policy, true_value =  tensor(61.2179)
1-hot policy, expected_value =  tensor(58.3596, dtype=torch.float64)
TRUE CONFUSION SCORE =  tensor(0.0382, dtype=torch.float64)
Time taken (seconds) =  0.2560720443725586
==========
trial_idx =  10
__file__ =  /home/local/ASUAD/sgopal28/workspace/AAAI20222_MDPminimizForHumans/src/SAMDP_policy_iteration_STANDARD_value_computation.py
Number of full state set iterations = 1
Number of full state set iterations = 2
Number of full state set iterations = 3
Number of full state set iterations = 4
Number of full state set iterations = 5
Number of full state set iterations = 6
policy matrix tensor([[0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]])
Time taken (seconds) =  0.25318217277526855
true_policy_matrix
tensor([[0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]])
[["('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)"]]
===============
weighted value =  tensor(6.4844, dtype=torch.float64)
1-hot policy, true_value =  tensor(61.2179)
1-hot policy, expected_value =  tensor(58.3596, dtype=torch.float64)
TRUE CONFUSION SCORE =  tensor(0.0382, dtype=torch.float64)
Time taken (seconds) =  0.25460147857666016
==========
trial_idx =  11
__file__ =  /home/local/ASUAD/sgopal28/workspace/AAAI20222_MDPminimizForHumans/src/SAMDP_policy_iteration_STANDARD_value_computation.py
Number of full state set iterations = 1
Number of full state set iterations = 2
Number of full state set iterations = 3
Number of full state set iterations = 4
Number of full state set iterations = 5
Number of full state set iterations = 6
Number of full state set iterations = 7
Number of full state set iterations = 8
policy matrix tensor([[0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.]])
Time taken (seconds) =  0.34056901931762695
true_policy_matrix
tensor([[0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.]])
[["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)"]]
===============
weighted value =  tensor(6.4844, dtype=torch.float64)
1-hot policy, true_value =  tensor(61.2179)
1-hot policy, expected_value =  tensor(58.3596, dtype=torch.float64)
TRUE CONFUSION SCORE =  tensor(0.0382, dtype=torch.float64)
Time taken (seconds) =  0.341961145401001
==========
trial_idx =  12
__file__ =  /home/local/ASUAD/sgopal28/workspace/AAAI20222_MDPminimizForHumans/src/SAMDP_policy_iteration_STANDARD_value_computation.py
Number of full state set iterations = 1
Number of full state set iterations = 2
Number of full state set iterations = 3
Number of full state set iterations = 4
Number of full state set iterations = 5
Number of full state set iterations = 6
Number of full state set iterations = 7
policy matrix tensor([[0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]])
Time taken (seconds) =  0.29787659645080566
true_policy_matrix
tensor([[0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]])
[["('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)"]]
===============
weighted value =  tensor(6.4844, dtype=torch.float64)
1-hot policy, true_value =  tensor(61.2179)
1-hot policy, expected_value =  tensor(58.3596, dtype=torch.float64)
TRUE CONFUSION SCORE =  tensor(0.0382, dtype=torch.float64)
Time taken (seconds) =  0.29927539825439453
==========
trial_idx =  13
__file__ =  /home/local/ASUAD/sgopal28/workspace/AAAI20222_MDPminimizForHumans/src/SAMDP_policy_iteration_STANDARD_value_computation.py
Number of full state set iterations = 1
Number of full state set iterations = 2
Number of full state set iterations = 3
Number of full state set iterations = 4
Number of full state set iterations = 5
Number of full state set iterations = 6
policy matrix tensor([[0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]])
Time taken (seconds) =  0.25751471519470215
true_policy_matrix
tensor([[0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]])
[["('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)"]]
===============
weighted value =  tensor(6.4844, dtype=torch.float64)
1-hot policy, true_value =  tensor(61.2179)
1-hot policy, expected_value =  tensor(58.3596, dtype=torch.float64)
TRUE CONFUSION SCORE =  tensor(0.0382, dtype=torch.float64)
Time taken (seconds) =  0.2589092254638672
==========
trial_idx =  14
__file__ =  /home/local/ASUAD/sgopal28/workspace/AAAI20222_MDPminimizForHumans/src/SAMDP_policy_iteration_STANDARD_value_computation.py
Number of full state set iterations = 1
Number of full state set iterations = 2
Number of full state set iterations = 3
Number of full state set iterations = 4
Number of full state set iterations = 5
Number of full state set iterations = 6
Number of full state set iterations = 7
policy matrix tensor([[0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.]])
Time taken (seconds) =  0.3050980567932129
true_policy_matrix
tensor([[0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.]])
[["('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)"]]
===============
weighted value =  tensor(6.4720, dtype=torch.float64)
1-hot policy, true_value =  tensor(61.2179)
1-hot policy, expected_value =  tensor(58.2479, dtype=torch.float64)
TRUE CONFUSION SCORE =  tensor(0.0493, dtype=torch.float64)
Time taken (seconds) =  0.3065035343170166
==========
trial_idx =  15
__file__ =  /home/local/ASUAD/sgopal28/workspace/AAAI20222_MDPminimizForHumans/src/SAMDP_policy_iteration_STANDARD_value_computation.py
Number of full state set iterations = 1
Number of full state set iterations = 2
Number of full state set iterations = 3
Number of full state set iterations = 4
Number of full state set iterations = 5
Number of full state set iterations = 6
Number of full state set iterations = 7
Number of full state set iterations = 8
policy matrix tensor([[0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]])
Time taken (seconds) =  0.34337401390075684
true_policy_matrix
tensor([[0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]])
[["('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)"]]
===============
weighted value =  tensor(6.4844, dtype=torch.float64)
1-hot policy, true_value =  tensor(61.2179)
1-hot policy, expected_value =  tensor(58.3596, dtype=torch.float64)
TRUE CONFUSION SCORE =  tensor(0.0382, dtype=torch.float64)
Time taken (seconds) =  0.34481048583984375
==========
trial_idx =  16
__file__ =  /home/local/ASUAD/sgopal28/workspace/AAAI20222_MDPminimizForHumans/src/SAMDP_policy_iteration_STANDARD_value_computation.py
Number of full state set iterations = 1
Number of full state set iterations = 2
Number of full state set iterations = 3
Number of full state set iterations = 4
Number of full state set iterations = 5
policy matrix tensor([[0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]])
Time taken (seconds) =  0.21497774124145508
true_policy_matrix
tensor([[0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]])
[["('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)"]]
===============
weighted value =  tensor(6.4720, dtype=torch.float64)
1-hot policy, true_value =  tensor(61.2179)
1-hot policy, expected_value =  tensor(58.2479, dtype=torch.float64)
TRUE CONFUSION SCORE =  tensor(0.0493, dtype=torch.float64)
Time taken (seconds) =  0.2164630889892578
==========
trial_idx =  17
__file__ =  /home/local/ASUAD/sgopal28/workspace/AAAI20222_MDPminimizForHumans/src/SAMDP_policy_iteration_STANDARD_value_computation.py
Number of full state set iterations = 1
Number of full state set iterations = 2
Number of full state set iterations = 3
Number of full state set iterations = 4
Number of full state set iterations = 5
Number of full state set iterations = 6
Number of full state set iterations = 7
policy matrix tensor([[0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]])
Time taken (seconds) =  0.2992587089538574
true_policy_matrix
tensor([[0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]])
[["('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)"]]
===============
weighted value =  tensor(6.4720, dtype=torch.float64)
1-hot policy, true_value =  tensor(61.2179)
1-hot policy, expected_value =  tensor(58.2479, dtype=torch.float64)
TRUE CONFUSION SCORE =  tensor(0.0493, dtype=torch.float64)
Time taken (seconds) =  0.3006582260131836
==========
trial_idx =  18
__file__ =  /home/local/ASUAD/sgopal28/workspace/AAAI20222_MDPminimizForHumans/src/SAMDP_policy_iteration_STANDARD_value_computation.py
Number of full state set iterations = 1
Number of full state set iterations = 2
Number of full state set iterations = 3
Number of full state set iterations = 4
Number of full state set iterations = 5
policy matrix tensor([[0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.]])
Time taken (seconds) =  0.2127842903137207
true_policy_matrix
tensor([[0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.]])
[["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)"]]
===============
weighted value =  tensor(6.4844, dtype=torch.float64)
1-hot policy, true_value =  tensor(61.2179)
1-hot policy, expected_value =  tensor(58.3596, dtype=torch.float64)
TRUE CONFUSION SCORE =  tensor(0.0382, dtype=torch.float64)
Time taken (seconds) =  0.21418333053588867
==========
trial_idx =  19
__file__ =  /home/local/ASUAD/sgopal28/workspace/AAAI20222_MDPminimizForHumans/src/SAMDP_policy_iteration_STANDARD_value_computation.py
Number of full state set iterations = 1
Number of full state set iterations = 2
Number of full state set iterations = 3
Number of full state set iterations = 4
Number of full state set iterations = 5
Number of full state set iterations = 6
Number of full state set iterations = 7
Number of full state set iterations = 8
policy matrix tensor([[0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]])
Time taken (seconds) =  0.3403642177581787
true_policy_matrix
tensor([[0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]])
[["('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)"]]
===============
weighted value =  tensor(6.4720, dtype=torch.float64)
1-hot policy, true_value =  tensor(61.2179)
1-hot policy, expected_value =  tensor(58.2479, dtype=torch.float64)
TRUE CONFUSION SCORE =  tensor(0.0493, dtype=torch.float64)
Time taken (seconds) =  0.34174084663391113
==========
trial_idx =  20
__file__ =  /home/local/ASUAD/sgopal28/workspace/AAAI20222_MDPminimizForHumans/src/SAMDP_policy_iteration_STANDARD_value_computation.py
Number of full state set iterations = 1
Number of full state set iterations = 2
Number of full state set iterations = 3
Number of full state set iterations = 4
Number of full state set iterations = 5
Number of full state set iterations = 6
Number of full state set iterations = 7
Number of full state set iterations = 8
policy matrix tensor([[0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]])
Time taken (seconds) =  0.34046173095703125
true_policy_matrix
tensor([[0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]])
[["('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)"]]
===============
weighted value =  tensor(6.4844, dtype=torch.float64)
1-hot policy, true_value =  tensor(61.2179)
1-hot policy, expected_value =  tensor(58.3596, dtype=torch.float64)
TRUE CONFUSION SCORE =  tensor(0.0382, dtype=torch.float64)
Time taken (seconds) =  0.34188365936279297
==========
trial_idx =  21
__file__ =  /home/local/ASUAD/sgopal28/workspace/AAAI20222_MDPminimizForHumans/src/SAMDP_policy_iteration_STANDARD_value_computation.py
Number of full state set iterations = 1
Number of full state set iterations = 2
Number of full state set iterations = 3
Number of full state set iterations = 4
Number of full state set iterations = 5
Number of full state set iterations = 6
policy matrix tensor([[0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.]])
Time taken (seconds) =  0.2964756488800049
true_policy_matrix
tensor([[0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.]])
[["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)"]]
===============
weighted value =  tensor(6.4844, dtype=torch.float64)
1-hot policy, true_value =  tensor(61.2179)
1-hot policy, expected_value =  tensor(58.3596, dtype=torch.float64)
TRUE CONFUSION SCORE =  tensor(0.0382, dtype=torch.float64)
Time taken (seconds) =  0.29800987243652344
==========
trial_idx =  22
__file__ =  /home/local/ASUAD/sgopal28/workspace/AAAI20222_MDPminimizForHumans/src/SAMDP_policy_iteration_STANDARD_value_computation.py
Number of full state set iterations = 1
Number of full state set iterations = 2
Number of full state set iterations = 3
Number of full state set iterations = 4
Number of full state set iterations = 5
Number of full state set iterations = 6
Number of full state set iterations = 7
Number of full state set iterations = 8
policy matrix tensor([[0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]])
Time taken (seconds) =  0.36649131774902344
true_policy_matrix
tensor([[0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]])
[["('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)"]]
===============
weighted value =  tensor(6.4844, dtype=torch.float64)
1-hot policy, true_value =  tensor(61.2179)
1-hot policy, expected_value =  tensor(58.3596, dtype=torch.float64)
TRUE CONFUSION SCORE =  tensor(0.0382, dtype=torch.float64)
Time taken (seconds) =  0.36798930168151855
==========
trial_idx =  23
__file__ =  /home/local/ASUAD/sgopal28/workspace/AAAI20222_MDPminimizForHumans/src/SAMDP_policy_iteration_STANDARD_value_computation.py
Number of full state set iterations = 1
Number of full state set iterations = 2
Number of full state set iterations = 3
Number of full state set iterations = 4
Number of full state set iterations = 5
Number of full state set iterations = 6
Number of full state set iterations = 7
policy matrix tensor([[0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]])
Time taken (seconds) =  0.3109462261199951
true_policy_matrix
tensor([[0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]])
[["('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)"]]
===============
weighted value =  tensor(6.4720, dtype=torch.float64)
1-hot policy, true_value =  tensor(61.2179)
1-hot policy, expected_value =  tensor(58.2479, dtype=torch.float64)
TRUE CONFUSION SCORE =  tensor(0.0493, dtype=torch.float64)
Time taken (seconds) =  0.312345027923584
==========
trial_idx =  24
__file__ =  /home/local/ASUAD/sgopal28/workspace/AAAI20222_MDPminimizForHumans/src/SAMDP_policy_iteration_STANDARD_value_computation.py
Number of full state set iterations = 1
Number of full state set iterations = 2
Number of full state set iterations = 3
Number of full state set iterations = 4
Number of full state set iterations = 5
Number of full state set iterations = 6
Number of full state set iterations = 7
policy matrix tensor([[0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.]])
Time taken (seconds) =  0.3291158676147461
true_policy_matrix
tensor([[0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.]])
[["('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)"]]
===============
weighted value =  tensor(6.4720, dtype=torch.float64)
1-hot policy, true_value =  tensor(61.2179)
1-hot policy, expected_value =  tensor(58.2479, dtype=torch.float64)
TRUE CONFUSION SCORE =  tensor(0.0493, dtype=torch.float64)
Time taken (seconds) =  0.33064985275268555
==========
trial_idx =  25
__file__ =  /home/local/ASUAD/sgopal28/workspace/AAAI20222_MDPminimizForHumans/src/SAMDP_policy_iteration_STANDARD_value_computation.py
Number of full state set iterations = 1
Number of full state set iterations = 2
Number of full state set iterations = 3
Number of full state set iterations = 4
Number of full state set iterations = 5
Number of full state set iterations = 6
Number of full state set iterations = 7
Number of full state set iterations = 8
policy matrix tensor([[0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]])
Time taken (seconds) =  0.40470242500305176
true_policy_matrix
tensor([[0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]])
[["('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)"]]
===============
weighted value =  tensor(6.4844, dtype=torch.float64)
1-hot policy, true_value =  tensor(61.2179)
1-hot policy, expected_value =  tensor(58.3596, dtype=torch.float64)
TRUE CONFUSION SCORE =  tensor(0.0382, dtype=torch.float64)
Time taken (seconds) =  0.40611958503723145
==========
trial_idx =  26
__file__ =  /home/local/ASUAD/sgopal28/workspace/AAAI20222_MDPminimizForHumans/src/SAMDP_policy_iteration_STANDARD_value_computation.py
Number of full state set iterations = 1
Number of full state set iterations = 2
Number of full state set iterations = 3
Number of full state set iterations = 4
Number of full state set iterations = 5
Number of full state set iterations = 6
Number of full state set iterations = 7
Number of full state set iterations = 8
policy matrix tensor([[0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]])
Time taken (seconds) =  0.3372812271118164
true_policy_matrix
tensor([[0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]])
[["('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)"]]
===============
weighted value =  tensor(6.4844, dtype=torch.float64)
1-hot policy, true_value =  tensor(61.2179)
1-hot policy, expected_value =  tensor(58.3596, dtype=torch.float64)
TRUE CONFUSION SCORE =  tensor(0.0382, dtype=torch.float64)
Time taken (seconds) =  0.3386564254760742
==========
trial_idx =  27
__file__ =  /home/local/ASUAD/sgopal28/workspace/AAAI20222_MDPminimizForHumans/src/SAMDP_policy_iteration_STANDARD_value_computation.py
Number of full state set iterations = 1
Number of full state set iterations = 2
Number of full state set iterations = 3
Number of full state set iterations = 4
Number of full state set iterations = 5
Number of full state set iterations = 6
policy matrix tensor([[0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.]])
Time taken (seconds) =  0.25495457649230957
true_policy_matrix
tensor([[0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.]])
[["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)"]]
===============
weighted value =  tensor(6.4844, dtype=torch.float64)
1-hot policy, true_value =  tensor(61.2179)
1-hot policy, expected_value =  tensor(58.3596, dtype=torch.float64)
TRUE CONFUSION SCORE =  tensor(0.0382, dtype=torch.float64)
Time taken (seconds) =  0.25632286071777344
==========
trial_idx =  28
__file__ =  /home/local/ASUAD/sgopal28/workspace/AAAI20222_MDPminimizForHumans/src/SAMDP_policy_iteration_STANDARD_value_computation.py
Number of full state set iterations = 1
Number of full state set iterations = 2
Number of full state set iterations = 3
Number of full state set iterations = 4
Number of full state set iterations = 5
Number of full state set iterations = 6
Number of full state set iterations = 7
Number of full state set iterations = 8
Number of full state set iterations = 9
policy matrix tensor([[0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]])
Time taken (seconds) =  0.38666367530822754
true_policy_matrix
tensor([[0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]])
[["('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)"]]
===============
weighted value =  tensor(6.4720, dtype=torch.float64)
1-hot policy, true_value =  tensor(61.2179)
1-hot policy, expected_value =  tensor(58.2479, dtype=torch.float64)
TRUE CONFUSION SCORE =  tensor(0.0493, dtype=torch.float64)
Time taken (seconds) =  0.38804078102111816
==========
trial_idx =  29
__file__ =  /home/local/ASUAD/sgopal28/workspace/AAAI20222_MDPminimizForHumans/src/SAMDP_policy_iteration_STANDARD_value_computation.py
Number of full state set iterations = 1
Number of full state set iterations = 2
Number of full state set iterations = 3
Number of full state set iterations = 4
Number of full state set iterations = 5
Number of full state set iterations = 6
policy matrix tensor([[0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]])
Time taken (seconds) =  0.26560306549072266
true_policy_matrix
tensor([[0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]])
[["('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)"]]
===============
weighted value =  tensor(6.4844, dtype=torch.float64)
1-hot policy, true_value =  tensor(61.2179)
1-hot policy, expected_value =  tensor(58.3596, dtype=torch.float64)
TRUE CONFUSION SCORE =  tensor(0.0382, dtype=torch.float64)
Time taken (seconds) =  0.26698803901672363
==========
 The best value from hill climbing was =  tensor(58.3596, dtype=torch.float64)
-------------BRANCH AND BOUND ---------------------------------

Using non-default solver options:
 - best_objective: tensor(57.3596, dtype=torch.float64) (default: -inf)

Starting branch & bound solve:
 - dispatcher pid: 3365800 (en4129772l)
 - worker processes: 1
--------------------------------------------------------------------------------------------------------------------------
         Nodes        |                      Objective Bounds                       |              Work              
      Expl    Unexpl  |      Incumbent           Bound    Rel. Gap         Abs. Gap | Time (s)  Nodes/Sec Imbalance   Idle
         0         1  |       57.35963             inf         inf%             inf |      0.0       0.00     0.00%      0
         1         4  |       57.35963        61.21869    6.727838%     3.859062682 |      0.0     286.95     0.00%      0
*      107        42  |       58.12918        58.54921    0.722586%    0.4200335652 |      0.4     300.66     0.00%      0
*      108        17  |       58.36118        58.54921    0.322186%    0.1880313069 |      0.4     739.14     0.00%      0
       125         0  |       58.36118        58.36118    0.000000%               0 |      0.4     871.60     0.00%      0
--------------------------------------------------------------------------------------------------------------------------

Absolute optimality tolerance met
Optimal solution found!

solver results:
 - solution_status: optimal
 - termination_condition: optimality
 - objective: 58.36118
 - bound: 58.36118
 - absolute_gap: 0
 - relative_gap: 0
 - nodes: 125
 - wall_time: 376.51 ms
 - best_node: Node(objective=58.36118)

Number of Workers:        1
Load Imbalance:       0.00%
 - min: 125 (proc rank=0)
 - max: 125 (proc rank=0)
Average Worker Timing:
 - queue:       2.43% [avg time:  73.3 us, count: 125]
 - load_state:  0.06% [avg time:   1.9 us, count: 125]
 - bound:      93.87% [avg time:   2.8 ms, count: 125]
 - objective:   0.64% [avg time:  51.0 us, count:  47]
 - branch:      0.36% [avg time:  30.4 us, count:  45]
 - other:       2.64% [avg time:  79.4 us, count: 125]

---BNB takes 0.37714695930480957 seconds ---
results from BnB with HASA BnB controller
tensor([[0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]])
[["('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)" "('ACTION_DOWN', 1.0)"]
 ["('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)" "('ACTION_RIGHT', 1.0)"]]
Expected Value with HASA BnB Controller = tensor(58.3596, dtype=torch.float64)
time taken for all settings =  9.620984077453613
